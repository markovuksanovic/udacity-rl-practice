{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this exercise is to teach an agent to collect bannanas. To do that we use a technique called \"reinforcement learning\" or RL for short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning algorithm\n",
    "\n",
    "Learning algorithm that we will use is called \"Deep Q Learning\" (DQN). The algorithm can be divided into tvo parts - sampling and learning. It works as follows:\n",
    "\n",
    "Sampling:\n",
    "1. Choose action A given state S using policy P.\n",
    "2. Take action A, observe reward R and next state S'.\n",
    "3. Store experience tuple (S, A, R, S') into replay memory M.\n",
    "\n",
    "Learning:\n",
    "4. Obtain random batch of tuples B from M.\n",
    "5. Set target y = r + discount_rate * max_a(q(S', a, w')\n",
    "6. Update Q network weights: delta_w = learning_rate * (y - q(S, a, w)) * grad_w(q(S, a, w))\n",
    "7. Every C steps set w' = w\n",
    "\n",
    "max_a(f(a)) - function that returns maximum value of f(a) with respect to a. \n",
    "\n",
    "grad_w(f(w)) - function that returns gradient of f(w) with respect to w.\n",
    "\n",
    "The algorithm tries to maximise expected cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "DQN used for this exercise is defined in `model.py` file. It is a simple neural network with two fully connected layers and ReLU activation in between.\n",
    "\n",
    "Number of neurons used in these fully connected layers is 64. As ve can see in the diagram below, this number of neurons allowed the neural network to achieve high enough score after some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code walkthrough\n",
    "\n",
    "Neural netvork is trained using `Navigation.ipynb` notebook. First we initialise the agent and then [start training process](Navigation-Marko.ipynb#Train-the-agent:)). During the training process we use $\\epsilon$ - greedy policy. In the beginning agent is very curious and takes actions randomly ($\\epsilon = 1.0$) but as time passes we decay $\\epsilon$ to 0.01 which means that agent is not very curious and mostly exploits existing knowledge.\n",
    "\n",
    "During the training process agent chooses an action to take and then we feed that action to the environment which in turn gives us a `state, action, next state, reward` tuple. This tuple is then given to `agent.step` method which is in charge of running training. `agent.step` method runs training every 10 steps (defined by `UPDATE_EVERY` parameter in `agent.py`) given memory buffer is filled with enough entries.\n",
    "\n",
    "Actual neural network training happens in `agent.learn` method. Two neural networks are used in this process - `qnetwork_target` and `qnetwork_local`. In that method we try to minimize difference between what our local network thinks each of the actions are worth and what the target network thinks.  In other words, we are trying to minimize so called \"TD error\".\n",
    "\n",
    "![TD error](images/qlearning-td-error.png)\n",
    "\n",
    "Target network is just a copy of the network which we are trying to train that is \"frozen\" in time and only updated every so many steps. This approach was popularised by Deep Mind in their DQN paper. It's called fixed Q targets. In this implementation instead of updating target network every X steps we use \"soft update\" method. These are not exactly the same methods but are quite similar. In case the agent failed to train this would be a good place to look for problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
